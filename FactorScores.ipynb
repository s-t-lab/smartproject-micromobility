{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import biogeme as biogeme\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme import models\n",
    "import biogeme.messaging as msgnotes\n",
    "from biogeme.expressions import Beta\n",
    "from biogeme.expressions import (\n",
    "    Beta,\n",
    "    DefineVariable,\n",
    "    bioDraws,\n",
    "    PanelLikelihoodTrajectory,\n",
    "    MonteCarlo,\n",
    "    log,\n",
    "    Derive,\n",
    "    bioNormalCdf,\n",
    "    Elem\n",
    ")\n",
    "import math\n",
    "from datetime import datetime\n",
    "from factor_analyzer import FactorAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import psychometric variables data\n",
    "psych_vars = pd.read_csv('Data/psych_vars.csv')\n",
    "\n",
    "# drop NA responses [3]\n",
    "psych_vars = psych_vars.dropna()\n",
    "psych_vars.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns from question numbers to variables\n",
    "\n",
    "psych_vars.rename(columns={'q16':'socialnorm1', 'q17':'socialnorm2', 'q18':'socialnorm3', 'q19':'socialnorm4', 'q20':'socialnorm5', 'q21':'socialnorm6'}, inplace=True)\n",
    "psych_vars.rename(columns={'q22': 'expectedperf1', 'q23': 'expectedperf2', 'q24': 'expectedperf3', 'q25': 'expectedperf4', 'q26': 'expectedperf5'}, inplace=True)\n",
    "psych_vars.rename(columns={'q27': 'envbenefit1', 'q28': 'envbenefit2', 'q29': 'envbenefit3', 'q30': 'envbenefit4'}, inplace=True)\n",
    "psych_vars.rename(columns={'q31': 'perceivedrisk1', 'q32': 'perceivedrisk2', 'q33': 'perceivedrisk3'}, inplace=True)\n",
    "psych_vars.rename(columns={'q34': 'resideffects1', 'q35': 'resideffects2', 'q36': 'resideffects3', 'q37': 'resideffects4'}, inplace=True)\n",
    "psych_vars.rename(columns={'q38': 'envvalues1', 'q39': 'envvalues2', 'q40': 'envvalues3', 'q41': 'envvalues4'}, inplace=True)\n",
    "psych_vars.rename(columns={'q42': 'antimicro1', 'q43': 'antimicro2', 'q44': 'antimicro3'}, inplace=True)\n",
    "psych_vars.rename(columns={'q45': 'intentiontouse1', 'q46': 'intentiontouse2', 'q47': 'intentiontouse3'}, inplace=True)\n",
    "\n",
    "# convert responses into numerical values with 6 as \"positive\" and \"1\" as negative attitudes towards micromobility\n",
    "\n",
    "# socialnorm1 --> \"my family or friends think using bikesharing or scootersharing is a positive thing\"\n",
    "psych_vars.replace({'socialnorm1': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# socialnorm 2 --> \"people important to me think that using bikesharing or scootersharing is a positive thing\"\n",
    "psych_vars.replace({'socialnorm2': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# socialnorm3 --> \"in the near future more people in my city will use bikesharing or scootersharing\"\n",
    "psych_vars.replace({'socialnorm3': {'Considerably fewer': 1, 'Fewer': 2, 'Slightly fewer': 3, 'Slightly more': 4, 'More': 5, 'A lot more': 6}}, inplace=True)\n",
    "# socialnorm4 --> \"people who are important to me think that I should use bikesharing or scooter sharing\"\n",
    "psych_vars.replace({\"socialnorm4\": {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# socialnorm5 --> \"it is a shame to use bikeshaing or scootersharing\"\n",
    "psych_vars.replace({'socialnorm5': {'Strongly agree': 1, 'Agree': 2, 'Slightly agree': 3, 'Slightly disagree': 4, 'Disagree': 5, 'Strongly disagree': 6}}, inplace=True)\n",
    "# socialnorm6 --> \"the social media evaluates bikesharing or scootersharing negatively\"\n",
    "psych_vars.replace({'socialnorm6': {'Very negative': 1, 'Negative': 2, 'Somewhat negative': 3, 'Somewhat positive': 4, 'Positive': 5, 'Very positive': 6}}, inplace=True)\n",
    "# expectedperf1 --> \"shared micromobility is convenient\"\n",
    "psych_vars.replace({'expectedperf1': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# expected perf2 --> \"shared micromobility is effective for my personal mobility\"\n",
    "psych_vars.replace({'expectedperf2': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# expectedperf3 --> \"shared micromobility can help me reach my destination efficiently\"\n",
    "psych_vars.replace({'expectedperf3': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# expectedperf4 --> \"there are enough shared bikes/scooters available whenever I want to use them\"\n",
    "psych_vars.replace({'expectedperf4': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# expectedperf5 --> \"I can comfortably take rides on a shared bike or scooter for my daily business\"\n",
    "psych_vars.replace({'expectedperf5': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# envbenefit1 --> \"using shared micromobility will help alleviate traffic congestion\"\n",
    "psych_vars.replace({'envbenefit1': {'Very unlikely': 1, 'Unlikely': 2, 'Slightly unlikely': 3, 'Slightly likely': 4, 'Likely': 5, 'Very likely': 6}}, inplace=True)\n",
    "# envbenefit2 --> \"using shared micromobility will reduce carbon emission and air pollution\"\n",
    "psych_vars.replace({'envbenefit2': {'Very unlikely': 1, 'Unlikely': 2, 'Slightly unlikely': 3, 'Slightly likely': 4, 'Likely': 5, 'Very likely': 6}}, inplace=True)\n",
    "# envbenefit3 --> \"using a shared bike or scooter fits my environmental concerns\"\n",
    "psych_vars.replace({'envbenefit3': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# envbenefit4 --> \"shared micromobility has a positive impact on urban traffic\"\n",
    "psych_vars.replace({'envbenefit4': {'Significantly increase': 1, 'Increase': 2, 'Slightly increase': 3, 'Slightly decrease': 4, 'Decrease': 5, 'Significantly decrease': 6}}, inplace=True)\n",
    "# perceivedrisk1 --> \"I would feel safe riding a shared bike or scooter in traffic\"\n",
    "psych_vars.replace({'perceivedrisk1': {'Very unsafe': 1, 'Unsafe': 2, 'Somewhat unsafe': 3, 'Somewhat safe': 4, 'Safe': 5, 'Very safe': 6}}, inplace=True)\n",
    "# perceivedrisk2 --> \"I think riding a shared bike or scooter is dangerous\"\n",
    "psych_vars.replace({'perceivedrisk2': {'Very dangerous': 1, 'Dangerous': 2, 'Somewhat dangerous': 3, 'Somewhat safe': 4, 'Safe': 5, 'Very safe': 6}}, inplace=True)\n",
    "# perceivedrisk3 --> \"I would feel nervous about having an accident when riding a shared bike or scooter\"\n",
    "psych_vars.replace({'perceivedrisk3': {'Strongly agree': 1, 'Agree': 2, 'Slightly agree': 3, 'Slightly disagree': 4, 'Disagree': 5, 'Strongly disagree': 6}}, inplace=True)\n",
    "# resideffects1 --> \"I knew about bikesharing or scootersharing before\"\n",
    "psych_vars.replace({'resideffects1': {'No': 0, 'Yes': 1}}, inplace=True)\n",
    "# resideffects2 --> \"many people around me know about bikesharing or scootersharing\"\n",
    "psych_vars.replace({'resideffects2': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# resideffects3 --> \"I have used bikesharing or scootersharing before\"\n",
    "psych_vars.replace({'resideffects3': {'No': 0, 'Yes': 1}}, inplace=True)\n",
    "# resideffects4 --> \"there are bikesharing or scootersharing available to me and I can use them regularly\"\n",
    "psych_vars.replace({'resideffects4': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# envvalues1 --> \"I would like to do my part to reduce carbon emission and air pollution\"\n",
    "psych_vars.replace({'envvalues1': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# envvalues2 --> \"I always consider how my transport choices affect the environment\"\n",
    "psych_vars.replace({'envvalues2': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# envvalues3 --> \"I consider myself to be an environmentally conscious person\"\n",
    "psych_vars.replace({'envvalues3': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# envvalues4 --> \"global warming is fake science\"\n",
    "psych_vars.replace({'envvalues4': {'Strongly agree': 1, 'Agree': 2, 'Slightly agree': 3, 'Slightly disagree': 4, 'Disagree': 5, 'Strongly disagree': 6}}, inplace=True)\n",
    "# F11 --> \"shared micromobility is a very bad idea\"\n",
    "psych_vars.replace({'antimicro1': {'Strongly agree': 1, 'Agree': 2, 'Slightly agree': 3, 'Slightly disagree': 4, 'Disagree': 5, 'Strongly disagree': 6}}, inplace=True)\n",
    "# F12 --> \"shared micromobility causes a lot of problems\"\n",
    "psych_vars.replace({'antimicro2': {'Strongly agree': 1, 'Agree': 2, 'Slightly agree': 3, 'Slightly disagree': 4, 'Disagree': 5, 'Strongly disagree': 6}}, inplace=True)\n",
    "# F13 --> \"shared micromobility should not have existed in cities\"\n",
    "psych_vars.replace({'antimicro3': {'Strongly agree': 1, 'Agree': 2, 'Slightly agree': 3, 'Slightly disagree': 4, 'Disagree': 5, 'Strongly disagree': 6}}, inplace=True)\n",
    "# intentiontouse1 --> \"I'm willing to use bikesharing or scootersharing in the future\"\n",
    "psych_vars.replace({'intentiontouse1': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n",
    "# intentiontouse2 --> \"I would recommend friends and family to use bikesharing or scootersharing\"\n",
    "psych_vars.replace({'intentiontouse2': {'Definetely would not': 1, 'Probably would not': 2, 'Maybe would not': 3, 'Maybe would': 4, 'Probably would': 5, 'Definetely would': 6}}, inplace=True)\n",
    "# intentiontouse3 --> \"I'm willing to use bikesharing or scootersharing on a regular basis\"\n",
    "psych_vars.replace({'intentiontouse3': {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3, 'Slightly agree': 4, 'Agree': 5, 'Strongly agree': 6}}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   socialnorm1  socialnorm2  socialnorm3  socialnorm4  socialnorm5  \\\n",
      "0            4            4            5            4            6   \n",
      "1            5            5            4            5            6   \n",
      "2            5            5            6            2            6   \n",
      "3            4            4            5            3            6   \n",
      "4            4            4            4            2            6   \n",
      "\n",
      "   socialnorm6  expectedperf1  expectedperf2  expectedperf3  expectedperf4  \\\n",
      "0            4              5              4              4              1   \n",
      "1            4              5              5              5              3   \n",
      "2            6              6              2              2              4   \n",
      "3            5              5              4              4              2   \n",
      "4            4              5              1              2              2   \n",
      "\n",
      "   ...  envvalues1  envvalues2  envvalues3  envvalues4  antimicro1  \\\n",
      "0  ...           5           5           5           6           6   \n",
      "1  ...           4           4           4           4           5   \n",
      "2  ...           5           6           6           5           5   \n",
      "3  ...           4           3           3           5           5   \n",
      "4  ...           5           2           4           6           6   \n",
      "\n",
      "   antimicro2  antimicro3  intentiontouse1  intentiontouse2  intentiontouse3  \n",
      "0           5           5                4                5                4  \n",
      "1           3           5                5                5                5  \n",
      "2           5           5                4                5                3  \n",
      "3           5           5                5                5                4  \n",
      "4           5           6                4                4                2  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# create new dataframes for factor analysis with and without user column since FA package does not work with user column\n",
    "factor_df_with_user = psych_vars[['user', 'socialnorm1', 'socialnorm2', 'socialnorm3', 'socialnorm4', 'socialnorm5', 'socialnorm6', 'expectedperf1', 'expectedperf2', 'expectedperf3', 'expectedperf4', 'expectedperf5', 'envbenefit1', 'envbenefit2', 'envbenefit3', 'envbenefit4', 'perceivedrisk1', 'perceivedrisk2', 'perceivedrisk3', 'resideffects1', 'resideffects2', 'resideffects3', 'resideffects4', 'envvalues1', 'envvalues2', 'envvalues3', 'envvalues4', 'antimicro1', 'antimicro2', 'antimicro3', 'intentiontouse1', 'intentiontouse2', 'intentiontouse3']]\n",
    "factor_df = psych_vars[['socialnorm1', 'socialnorm2', 'socialnorm3', 'socialnorm4', 'socialnorm5', 'socialnorm6', 'expectedperf1', 'expectedperf2', 'expectedperf3', 'expectedperf4', 'expectedperf5', 'envbenefit1', 'envbenefit2', 'envbenefit3', 'envbenefit4', 'perceivedrisk1', 'perceivedrisk2', 'perceivedrisk3', 'resideffects1', 'resideffects2', 'resideffects3', 'resideffects4', 'envvalues1', 'envvalues2', 'envvalues3', 'envvalues4', 'antimicro1', 'antimicro2', 'antimicro3', 'intentiontouse1', 'intentiontouse2', 'intentiontouse3']]\n",
    "\n",
    "# check\n",
    "print(factor_df.head())\n",
    "\n",
    "# perform factor analysis with promax rotation, 6 factors, maximum likelihood method\n",
    "fa = FactorAnalyzer(rotation='promax', n_factors=6, method = 'ml')\n",
    "fa.fit(factor_df)\n",
    "\n",
    "# store the factor scores\n",
    "scores = fa.transform(factor_df)\n",
    "\n",
    "# extract the user column\n",
    "user_column = factor_df_with_user['user']\n",
    "\n",
    "# create a new DataFrame with factor scores and 'user' column\n",
    "factor_scores_df = pd.DataFrame(scores, columns=['Factor_1', 'Factor_2', 'Factor_3', 'Factor_4', 'Factor_5', 'Factor_6']) #, 'Factor_7', 'Factor_8'])\n",
    "factor_scores_df['user'] = user_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge factor_scores_df and psych_vars on 'user'\n",
    "psych_vars_with_factors = pd.merge(psych_vars, factor_scores_df, on = 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import micromobility stated preference experiment data\n",
    "micro_pool = pd.read_csv('Data/micro_pool_socio_bio2up.csv')\n",
    "\n",
    "# Import scoot_user_id.csv\n",
    "scoot_user_id = pd.read_csv('Data/scoot_user_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_pool['unique_id'] = micro_pool['p'].astype(str) + '_' + micro_pool['dperson'].astype(str)\n",
    "scoot_user_id['unique_id'] = scoot_user_id['p'].astype(str) + '_' + scoot_user_id['dperson'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>p</th>\n",
       "      <th>dperson</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>862</td>\n",
       "      <td>5FnoeQPy6YG4B2uPYVv5</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>5_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2416</td>\n",
       "      <td>DgTbEDQDHZ5R4LzrlCPo</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>3997</td>\n",
       "      <td>IklQCaPqMOhsE3KVFiuy</td>\n",
       "      <td>5</td>\n",
       "      <td>209</td>\n",
       "      <td>5_209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>6937</td>\n",
       "      <td>SuQSNCfLa6DUBAOe02Dw</td>\n",
       "      <td>5</td>\n",
       "      <td>241</td>\n",
       "      <td>5_241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                  user  p  dperson unique_id\n",
       "169          862  5FnoeQPy6YG4B2uPYVv5  5       45      5_45\n",
       "480         2416  DgTbEDQDHZ5R4LzrlCPo  5       13      5_13\n",
       "790         3997  IklQCaPqMOhsE3KVFiuy  5      209     5_209\n",
       "1380        6937  SuQSNCfLa6DUBAOe02Dw  5      241     5_241"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# micro_pool_socio[(micro_pool_socio['p'] == 5)]\n",
    "scoot_user_id[(scoot_user_id['p'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# merge micro_pool_socio and scooter_user_id on 'p'\n",
    "micro_pool_socio = pd.merge(micro_pool, scoot_user_id, on='unique_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset for only micromobility mode choices\n",
    "\n",
    "micro0to5 = (((\n",
    "# 0-2 miles first choice\n",
    "micro_pool_socio['scooter_av'] == 1) & (\n",
    "    micro_pool_socio['dlbike_av'] == 1) & (\n",
    "    micro_pool_socio['dkbike_av'] == 1) & (\n",
    "    micro_pool_socio['car_av'] == 0) & (\n",
    "    micro_pool_socio['transit_av'] == 0) & (\n",
    "    micro_pool_socio['rd_av'] == 0) & (\n",
    "    micro_pool_socio['walk_av'] == 0) & (\n",
    "    micro_pool_socio['bike_av'] == 0) & (\n",
    "    micro_pool_socio['sctransit_av'] == 0)) | ((\n",
    "# 0-2 miles second choice\n",
    "    # scooter_av == 1 & dlbike_av == 1 & everything else == 0\n",
    "    micro_pool_socio['scooter_av'] == 1) & (\n",
    "    micro_pool_socio['dlbike_av'] == 1) & (\n",
    "    micro_pool_socio['dkbike_av'] == 0) & (\n",
    "    micro_pool_socio['car_av'] == 0) & (\n",
    "    micro_pool_socio['transit_av'] == 0) & (\n",
    "    micro_pool_socio['rd_av'] == 0) & (\n",
    "    micro_pool_socio['walk_av'] == 0) & (\n",
    "    micro_pool_socio['bike_av'] == 0) & (\n",
    "    micro_pool_socio['sctransit_av'] == 0)) | ((\n",
    "    #'scooter'_av ==1 & 'dk_bike'_av == 1 & everything else == 0\n",
    "    micro_pool_socio['scooter_av'] == 1) & (\n",
    "    micro_pool_socio['dkbike_av'] == 1) & (\n",
    "    micro_pool_socio['dlbike_av'] == 0) & (\n",
    "    micro_pool_socio['car_av'] == 0) & (\n",
    "    micro_pool_socio['transit_av'] == 0) & (\n",
    "    micro_pool_socio['rd_av'] == 0) & (\n",
    "    micro_pool_socio['walk_av'] == 0) & (\n",
    "    micro_pool_socio['bike_av'] == 0) & (\n",
    "    micro_pool_socio['sctransit_av'] == 0)) | ((\n",
    "    #'dk_bike_av' == 1 & 'dl_bike_av' == 1 & everything else == 0\n",
    "    micro_pool_socio['dkbike_av'] == 1) & (\n",
    "    micro_pool_socio['dlbike_av'] == 1) & (\n",
    "    micro_pool_socio['scooter_av'] == 0) & (\n",
    "    micro_pool_socio['car_av'] == 0) & (\n",
    "    micro_pool_socio['transit_av'] == 0) & (\n",
    "    micro_pool_socio['rd_av'] == 0) & (\n",
    "    micro_pool_socio['walk_av'] == 0) & (\n",
    "    micro_pool_socio['bike_av'] == 0) & (\n",
    "    micro_pool_socio['sctransit_av'] == 0\n",
    "    )) | ((\n",
    "# 2-5 miles\n",
    "    micro_pool_socio['scooter_av'] == 1) & (\n",
    "    micro_pool_socio['dlbike_av'] == 0) & (\n",
    "    micro_pool_socio['dkbike_av'] == 0) & (\n",
    "    micro_pool_socio['car_av'] == 0) & (\n",
    "    micro_pool_socio['transit_av'] == 0) & (\n",
    "    micro_pool_socio['rd_av'] == 0) & (\n",
    "    micro_pool_socio['walk_av'] == 0) & (\n",
    "    micro_pool_socio['bike_av'] == 0) & (\n",
    "    micro_pool_socio['sctransit_av'] == 1\n",
    ")))\n",
    "\n",
    "micro_0_to_5 = micro_pool_socio.loc[micro0to5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge psych_vars and micro_pool_socio using inner join, thus dropping any users who did not answer psychometric questions\n",
    "combined_psych = pd.merge(micro_0_to_5, psych_vars_with_factors, on='user', )\n",
    "\n",
    "# get count of unique users\n",
    "combined_psych['user'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_psych['who'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sort by dperson\n",
    "combined_psych = combined_psych.sort_values(by=['who'])\n",
    "\n",
    "# drop user column bc is it is type string\n",
    "combined_psych = combined_psych.drop(columns=['user'])\n",
    "combined_psych = combined_psych.drop(columns=['unique_id'])\n",
    "\n",
    "# convert to database\n",
    "database = db.Database('combined_psych', combined_psych)\n",
    "# panel data\n",
    "database.panel(\"who\")\n",
    "# create global variables\n",
    "globals().update(database.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852\n"
     ]
    }
   ],
   "source": [
    "# print the number of unique entries in the \"who\" column of combined_psych\n",
    "print(combined_psych['who'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 18\n",
      "Median: 35.0\n",
      "Mean: 37.97\n",
      "Max: 84\n",
      "Female: 42.6%\n",
      "Male: 56.7%\n",
      "Another: 0.7%\n",
      "White: 80.8%\n",
      "Black or African American: 8.8%\n",
      "American Indian or Alaska Native: 1.3%\n",
      "Asian: 6.5%\n",
      "Native Hawaiian or Other Pacific Islander: 0.1%\n",
      "Less than bachelor’s degree: 31.7%\n",
      "Bachelor’s degree or higher: 68.3%\n",
      "Employed: 89.6%\n",
      "Not employed: 10.4%\n",
      "Under $25,000: 15.8%\n",
      "$25,000-$49,999: 25.9%\n",
      "$50,000-$74,999: 25.9%\n",
      "$75,000-$99,999: 16.5%\n",
      "$100,000-$149,999: 9.9%\n",
      "≥  $150,000: 5.9%\n"
     ]
    }
   ],
   "source": [
    "# create a new dataframe with only the unique 'who' values in combined_psych\n",
    "respondents = combined_psych.drop_duplicates(subset=['who']).copy()\n",
    "\n",
    "total_rows = (combined_psych['who'].nunique())\n",
    "\n",
    "# calculate summary statistics for 'age' variable\n",
    "age_min = respondents['age'].min()\n",
    "age_median = respondents['age'].median()\n",
    "age_mean = respondents['age'].mean()\n",
    "age_max = respondents['age'].max()\n",
    "\n",
    "# print the summary statistics\n",
    "print(f\"Min: {age_min}\")\n",
    "print(f\"Median: {age_median}\")\n",
    "print(f\"Mean: {age_mean:.2f}\")\n",
    "print(f\"Max: {age_max}\")\n",
    "\n",
    "# calculate summary statistics for the remaining socio-demographic variables\n",
    "gender_F_percentage = (respondents['gender_F'].sum() / total_rows * 100)\n",
    "gender_M_percentage = (respondents['gender_M'].sum() / total_rows) * 100\n",
    "gender_O_percentage = (respondents['gender_O'].sum() / total_rows) * 100\n",
    "race_white_percentage = (respondents['race'] == 1).sum() / total_rows * 100\n",
    "race_black_percentage = (respondents['race'] == 2).sum() / total_rows * 100\n",
    "race_american_indian_percentage = (respondents['race'] == 3).sum() / total_rows * 100\n",
    "race_asian_percentage = (respondents['race'] == 4).sum() / total_rows * 100\n",
    "race_pacific_percentage = (respondents['race'] == 5).sum() / total_rows * 100\n",
    "education_less_than_bach = (respondents['edu'] < 5).sum() / total_rows * 100\n",
    "education_bachelor_higher_percentage = (respondents['edu'] >= 5).sum() / total_rows * 100\n",
    "employment_employed_percentage = (respondents['work'] <= 2).sum() / total_rows * 100\n",
    "employment_not_employed_percentage = (respondents['work'] > 2).sum() / total_rows * 100\n",
    "income_under_25k_percentage = (respondents['hhincome'] <= 4).sum() / total_rows * 100\n",
    "income_25k_50k_percentage = ((respondents['hhincome'] == 5) | (respondents['hhincome'] == 6)).sum() / total_rows * 100\n",
    "income_50k_75k_percentage = ((respondents['hhincome'] == 7)).sum() / total_rows * 100\n",
    "income_75k_100k_percentage = (respondents['hhincome'] == 8).sum() / total_rows * 100\n",
    "income_100k_150k_percentage = (respondents['hhincome'] == 9).sum() / total_rows * 100\n",
    "income_above_150k_percentage = (respondents['hhincome'] >= 10).sum() / total_rows * 100\n",
    "\n",
    "\n",
    "# print the summary statistics\n",
    "print(f\"Female: {gender_F_percentage:.1f}%\")\n",
    "print(f\"Male: {gender_M_percentage:.1f}%\")\n",
    "print(f\"Another: {gender_O_percentage:.1f}%\")\n",
    "print(f\"White: {race_white_percentage:.1f}%\")\n",
    "print(f\"Black or African American: {race_black_percentage:.1f}%\")\n",
    "print(f\"American Indian or Alaska Native: {race_american_indian_percentage:.1f}%\")\n",
    "print(f\"Asian: {race_asian_percentage:.1f}%\")\n",
    "print(f\"Native Hawaiian or Other Pacific Islander: {race_pacific_percentage:.1f}%\")\n",
    "print(f\"Less than bachelor’s degree: {education_less_than_bach:.1f}%\")\n",
    "print(f\"Bachelor’s degree or higher: {education_bachelor_higher_percentage:.1f}%\")\n",
    "print(f\"Employed: {employment_employed_percentage:.1f}%\")\n",
    "print(f\"Not employed: {employment_not_employed_percentage:.1f}%\")\n",
    "print(f\"Under $25,000: {income_under_25k_percentage:.1f}%\")\n",
    "print(f\"$25,000-$49,999: {income_25k_50k_percentage:.1f}%\")\n",
    "print(f\"$50,000-$74,999: {income_50k_75k_percentage:.1f}%\")\n",
    "print(f\"$75,000-$99,999: {income_75k_100k_percentage:.1f}%\")\n",
    "print(f\"$100,000-$149,999: {income_100k_150k_percentage:.1f}%\")\n",
    "print(f\"≥  $150,000: {income_above_150k_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_SCOOTERTIME = Beta('B_SCOOTERTIME', 0, None, None, 0)\n",
    "B_BIKETIME = Beta('B_BIKETIME', 0, None, None, 0)\n",
    "B_SCTRANSITTIME = Beta('B_SCTRANSITTIME', 0, None, None, 0)\n",
    "\n",
    "B_ACCESS = Beta('B_ACCESS', 0, None, None, 0)\n",
    "B_DROP =Beta('B_DROP', 0, None, None, 0)\n",
    "B_WAITAV = Beta('B_WAITAV', 0, None, None, 0)\n",
    "B_AV = Beta('B_AV', 0, None, None, 0)\n",
    "\n",
    "B_COST_ADJ = Beta('B_COST_ADJ', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercepts\n",
    "ASC_SCOOTER = Beta('ASC_SCOOTER', 0, None, None, 1)\n",
    "\n",
    "ASC_DKBIKE = Beta('ASC_DKBIKE', 0, None, None, 0)\n",
    "ASC_DKBIKE_S = Beta('ASC_DKBIKE_S', 1, None, None, 0)\n",
    "ASC_DKBIKE_RND = ASC_DKBIKE + ASC_DKBIKE_S * bioDraws('ASC_DKBIKE_RND', 'NORMAL_ANTI')\n",
    "\n",
    "ASC_DLBIKE = Beta('ASC_DLBIKE', 0, None, None, 0)\n",
    "ASC_DLBIKE_S = Beta('ASC_DLBIKE_S', 1, None, None, 0)\n",
    "ASC_DLBIKE_RND = ASC_DLBIKE + ASC_DLBIKE_S * bioDraws('ASC_DLBIKE_RND', 'NORMAL_ANTI')\n",
    "\n",
    "ASC_SCTRANSIT = Beta('ASC_SCTRANSIT', 0, None, None, 0)\n",
    "ASC_SCTRANSIT_S = Beta('ASC_SCTRANSIT_S', 1, None, None, 0)\n",
    "ASC_SCTRANSIT_RND = ASC_SCTRANSIT + ASC_SCTRANSIT_S * bioDraws('ASC_SCTRANSIT_RND', 'NORMAL_ANTI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor score coefficients\n",
    "B_F1_DLBIKE = Beta('B_F1_DLBIKE', 0, None, None, 0)\n",
    "B_F2_DLBIKE = Beta('B_F2_DLBIKE', 0, None, None, 0)\n",
    "B_F3_DLBIKE = Beta('B_F3_DLBIKE', 0, None, None, 0)\n",
    "B_F4_DLBIKE = Beta('B_F4_DLBIKE', 0, None, None, 0)\n",
    "B_F5_DLBIKE = Beta('B_F5_DLBIKE', 0, None, None, 0)\n",
    "B_F6_DLBIKE = Beta('B_F6_DLBIKE', 0, None, None, 0)\n",
    "\n",
    "B_F1_DKBIKE = Beta('B_F1_DKBIKE', 0, None, None, 0)\n",
    "B_F2_DKBIKE = Beta('B_F2_DKBIKE', 0, None, None, 0)\n",
    "B_F3_DKBIKE = Beta('B_F3_DKBIKE', 0, None, None, 0)\n",
    "B_F4_DKBIKE = Beta('B_F4_DKBIKE', 0, None, None, 0)\n",
    "B_F5_DKBIKE = Beta('B_F5_DKBIKE', 0, None, None, 0)\n",
    "B_F6_DKBIKE = Beta('B_F6_DKBIKE', 0, None, None, 0)\n",
    "\n",
    "B_F1_SCTRANSIT = Beta('B_F1_SCTRANSIT', 0, None, None, 0)\n",
    "B_F2_SCTRANSIT = Beta('B_F2_SCTRANSIT', 0, None, None, 0)\n",
    "B_F3_SCTRANSIT = Beta('B_F3_SCTRANSIT', 0, None, None, 0)\n",
    "B_F4_SCTRANSIT = Beta('B_F4_SCTRANSIT', 0, None, None, 0)\n",
    "B_F5_SCTRANSIT = Beta('B_F5_SCTRANSIT', 0, None, None, 0)\n",
    "B_F6_SCTRANSIT = Beta('B_F6_SCTRANSIT', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip purpose coefficients\n",
    "B_DLBIKE_NHB = Beta('B_DLBIKE_NHB', 0, None, None, 0)\n",
    "B_DKBIKE_NHB = Beta('B_DKBIKE_NHB', 0, None, None, 0)\n",
    "B_SCTRANSIT_NHB = Beta('B_SCTRANSIT_NHB', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "V7 = (ASC_SCOOTER\n",
    "      + B_SCOOTERTIME*sctime\n",
    "      + B_COST_ADJ*sccost_adj\n",
    "      + B_ACCESS*SCAW*(1-AVtech)\n",
    "      + B_WAITAV*SCAV*AVtech\n",
    "      )\n",
    "\n",
    "V8 = (ASC_DLBIKE_RND\n",
    "      + B_BIKETIME*dltime\n",
    "      + B_COST_ADJ*dlcost_adj\n",
    "      + B_ACCESS*DLAW\n",
    "      + B_F4_DLBIKE*Factor_4\n",
    "      + B_F5_DLBIKE*Factor_5\n",
    "      + B_DLBIKE_NHB*trippurp_NHB\n",
    "      )\n",
    "\n",
    "V9 = (ASC_DKBIKE_RND\n",
    "      + B_BIKETIME*dbtime\n",
    "      + B_COST_ADJ*dbcost_adj\n",
    "      + B_ACCESS*DBAW\n",
    "      + B_DROP*DBDW\n",
    "      + B_F4_DKBIKE*Factor_4\n",
    "      + B_F5_DKBIKE*Factor_5\n",
    "      + B_DKBIKE_NHB*trippurp_NHB\n",
    "      )\n",
    "\n",
    "V10 = (ASC_SCTRANSIT_RND\n",
    "       + B_SCTRANSITTIME*sttotime\n",
    "       + B_COST_ADJ*sttocost_adj\n",
    "       + B_ACCESS*STAW*(1-AVtech)\n",
    "       + B_WAITAV*STAV*AVtech\n",
    "       + B_F4_SCTRANSIT*Factor_4\n",
    "       + B_F5_SCTRANSIT*Factor_5\n",
    "       + B_SCTRANSIT_NHB*trippurp_NHB\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of argument numberOfThreads in the constructor of the BIOGEME object is deprecated and will be removed in future versions of Biogeme. Instead, define parameter number_of_threads in section MultiThreadingof the .toml parameter file. The default file name is biogeme.toml\n",
      "The use of argument numberOfDraws in the constructor of the BIOGEME object is deprecated and will be removed in future versions of Biogeme. Instead, define parameter number_of_draws in section MonteCarloof the .toml parameter file. The default file name is biogeme.toml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated betas: 22\n",
      "Final log likelihood: -2500.626\n",
      "                 Value  Rob. Std err  Rob. t-test  Rob. p-value\n",
      "ASC_DKBIKE       -0.43          0.24        -1.77          0.08\n",
      "ASC_DKBIKE_S      1.38          0.17         8.11          0.00\n",
      "ASC_DLBIKE        0.33          0.21         1.56          0.12\n",
      "ASC_DLBIKE_S      0.93          0.16         5.93          0.00\n",
      "ASC_SCTRANSIT    -1.20          0.41        -2.92          0.00\n",
      "ASC_SCTRANSIT_S   1.92          0.17        11.27          0.00\n",
      "B_ACCESS         -0.16          0.02        -8.18          0.00\n",
      "B_BIKETIME       -0.08          0.03        -2.68          0.01\n",
      "B_COST_ADJ       -2.60          0.46        -5.67          0.00\n",
      "B_DKBIKE_NHB     -0.65          0.29        -2.27          0.02\n",
      "B_DLBIKE_NHB     -0.39          0.22        -1.80          0.07\n",
      "B_DROP           -0.11          0.04        -2.92          0.00\n",
      "B_F4_DKBIKE       0.36          0.18         2.01          0.04\n",
      "B_F4_DLBIKE       0.40          0.14         2.81          0.00\n",
      "B_F4_SCTRANSIT   -0.20          0.15        -1.31          0.19\n",
      "B_F5_DKBIKE      -0.32          0.18        -1.81          0.07\n",
      "B_F5_DLBIKE      -0.35          0.13        -2.71          0.01\n",
      "B_F5_SCTRANSIT   -0.04          0.16        -0.26          0.80\n",
      "B_SCOOTERTIME    -0.08          0.02        -4.53          0.00\n",
      "B_SCTRANSITTIME  -0.01          0.03        -0.26          0.80\n",
      "B_SCTRANSIT_NHB  -0.79          0.24        -3.34          0.00\n",
      "B_WAITAV         -0.01          0.03        -0.41          0.68\n"
     ]
    }
   ],
   "source": [
    "\n",
    "V = {7: V7, 8: V8, 9: V9, 10: V10}\n",
    "av = {7: scooter_av, 8: dlbike_av, 9: dkbike_av, 10: sctransit_av}\n",
    "    \n",
    "\n",
    "condprob = models.logit(V, av, choice)\n",
    "condlikeindiv = PanelLikelihoodTrajectory(condprob)\n",
    "logprob = log(MonteCarlo(condlikeindiv))\n",
    "\n",
    "biogeme = bio.BIOGEME(database, logprob, numberOfDraws = 1000, numberOfThreads = 100)\n",
    "biogeme.modelName = \"MixedLogit with Factor Scores, trip purpose -Micro Only 1000 draws\"\n",
    "\n",
    "results = biogeme.estimate()\n",
    "pandasResults = results.getEstimatedParameters()\n",
    "print(f\"Estimated betas: {len(results.data.betaValues)}\")\n",
    "print(f\"Final log likelihood: {results.data.logLike:.3f}\")\n",
    "print(pandasResults.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
